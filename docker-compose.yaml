---
services:
  # ============================================================================
  # Database - ScyllaDB (NoSQL)
  # ============================================================================
  scylladb:
    image: scylladb/scylla:5.4.0
    container_name: scylladb
    restart: always
    networks:
      - chaty-dev
    ports:
      - "9042:9042"
      - "9160:9160"
      - "10000:10000"
      - "19042:19042"
    volumes:
      - scylladb_data:/var/lib/scylla
    command: --smp 1 --memory 750M
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "DESCRIBE keyspaces"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"

  # ============================================================================
  # Database - CockroachDB
  # ============================================================================
  cockroachdb:
    image: cockroachdb/cockroach:latest
    container_name: cockroachdb
    restart: always
    networks:
      - chaty-dev
    command: start-single-node --insecure
    ports:
      - "26257:26257"
      - "8190:8080"
    volumes:
      - cockroach_data:/cockroach/cockroach-data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health?ready=1"]
      interval: 5s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # ============================================================================
  # API Gateway - Envoy Proxy
  # ============================================================================
  chaty_proxy:
    image: envoyproxy/envoy:v1.35.0
    container_name: chaty-proxy
    networks:
      - chaty-dev
    restart: always
    volumes:
      - ./config/envoy.yml:/etc/envoy/envoy.yaml
      - ./bin/chaty_envoy_filter.wasm:/etc/envoy/wasm/chaty_envoy_filter.wasm:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command:
      [
        "-c",
        "/etc/envoy/envoy.yaml",
        "--component-log-level",
        "jwt:debug,http:debug,filter:debug",
      ]
    ports:
      - "8091:8091" # next api server
      - "8092:8092" # next web grpc
      - "9905:9905" # Envoy admin port
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.3"

  # ============================================================================
  # Cache - Dragonfly (Redis-compatible)
  # ============================================================================
  dragonfly:
    image: "docker.dragonflydb.io/dragonflydb/dragonfly"
    container_name: dragonfly
    restart: always
    ulimits:
      memlock: -1
    networks:
      - chaty-dev
    ports:
      - "6379:6379"
    volumes:
      - dragonfly_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"

  # ============================================================================
  # Message Broker - Redpanda (Kafka compatible)
  # ============================================================================
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    container_name: redpanda
    restart: always
    networks:
      - chaty-dev
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    ports:
      - "19092:19092"
      - "9644:9644"
    command:
      - redpanda
      - start
      - --mode dev-container
      - --smp 1
      - --memory 1G
      - --reserve-memory 0M
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
    healthcheck:
      test:
        ["CMD-SHELL", "curl -f http://localhost:9644/v1/node_config || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 90s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"

  # ============================================================================
  # Object Storage - MinIO (S3-compatible)
  # ============================================================================
  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    hostname: minio
    restart: always
    networks:
      - chaty-dev
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: chaty-dev
      MINIO_ROOT_PASSWORD: chaty-dev-password
      MINIO_CONSOLE_ADDRESS: ":9001"
    command: server /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # ============================================================================
  # MinIO Bucket Creation
  # ============================================================================
  createbuckets:
    image: minio/mc:latest
    container_name: createbuckets
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - chaty-dev
    entrypoint: >
      /bin/sh -c "
      until /usr/bin/mc ready minio; do
        /usr/bin/mc alias set minio http://minio:9000 chaty-dev chaty-dev-password;
        echo 'Waiting for MinIO...' && sleep 1;
      done;
      /usr/bin/mc mb minio/chaty-uploads;
      /usr/bin/mc mb minio/chaty-avatars;
      /usr/bin/mc mb minio/chaty-files;
      exit 0;
      "
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.2"

  # ============================================================================
  # Mail Server - MailDev (for development)
  # ============================================================================
  maildev:
    image: soulteary/maildev:latest
    container_name: maildev
    restart: always
    networks:
      - chaty-dev
    ports:
      - "1025:25"
      - "8025:8080"
    environment:
      MAILDEV_SMTP_PORT: 25
      MAILDEV_WEB_PORT: 8080
      MAILDEV_INCOMING_USER: smtp
      MAILDEV_INCOMING_PASS: smtp
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.2"

  # ============================================================================
  # OAuth 2.0 Authorization Server - Hydra (Database)
  # ============================================================================
  hydra-db:
    image: postgres:15
    container_name: hydra-db
    restart: always
    networks:
      - chaty-dev
    environment:
      POSTGRES_DB: hydra
      POSTGRES_USER: hydra
      POSTGRES_PASSWORD: hydra-password
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - hydra_db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hydra -d hydra"]
      interval: 5s
      timeout: 5s
      retries: 10
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # ============================================================================
  # OAuth 2.0 Authorization Server - Hydra (Migration)
  # ============================================================================
  hydra-migrate:
    image: oryd/hydra:v2.2.0
    container_name: hydra-migrate
    networks:
      - chaty-dev
    depends_on:
      hydra-db:
        condition: service_healthy
    environment:
      DSN: postgres://hydra:hydra-password@hydra-db:5432/hydra?sslmode=disable
    command: ["migrate", "sql", "-e", "--yes"]
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.3"

  # ============================================================================
  # OAuth 2.0 Authorization Server - Hydra
  # ============================================================================
  hydra:
    image: oryd/hydra:v2.2.0
    container_name: hydra
    restart: always
    networks:
      - chaty-dev
    depends_on:
      hydra-migrate:
        condition: service_completed_successfully
    ports:
      - "4444:4444"
      - "4445:4445"
    environment:
      DSN: postgres://hydra:hydra-password@hydra-db:5432/hydra?sslmode=disable
      HYDRA_SYSTEM_SECRET: this_needs_to_be_32_bytes_minimum_secret_key_12345
    volumes:
      - ./config/hydra.yml:/config/hydra.yml:ro
    command: ["serve", "all", "--dev", "--config", "/config/hydra.yml"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4444/health/ready"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # ============================================================================
  # Kafka UI (Optional - for Redpanda monitoring)
  # ============================================================================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    networks:
      - chaty-dev
    ports:
      - "8080:8080"
    depends_on:
      redpanda:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: "redpanda-local"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "redpanda:9092"
      KAFKA_CLUSTERS_0_ZOOKEEPER: ""
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: "http://redpanda:8081"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.3"

  # ============================================================================
  # Observability Stack - Jaeger (must come before otel-collector)
  # ============================================================================
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    restart: always
    networks:
      - chaty-dev
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=true
    volumes:
      - jaeger_data:/badger
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:14268"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # ============================================================================
  # Observability Stack - OpenTelemetry Collector
  # ============================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib
    container_name: otel-collector
    restart: always
    networks:
      - chaty-dev
    command: ["--config=/etc/otel-collector-config.yaml"]
    ports:
      - "4317:4317"
      - "4318:4318"
      - "8888:8888"
      - "13133:13133"
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    depends_on:
      jaeger:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 300M
          cpus: "0.5"

  # ============================================================================
  # Observability Stack - Prometheus
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: always
    networks:
      - chaty-dev
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus-rules.yaml:/etc/prometheus/rules/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--web.enable-lifecycle"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 300M
          cpus: "0.5"

  # ============================================================================
  # Observability Stack - Loki
  # ============================================================================
  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: always
    networks:
      - chaty-dev
    ports:
      - "3101:3101"
    volumes:
      - ./config/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3101/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.3"

  # ============================================================================
  # Observability Stack - Promtail
  # ============================================================================
  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    restart: always
    networks:
      - chaty-dev
    volumes:
      - ./config/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/config.yaml
    depends_on:
      loki:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.2"

  # ============================================================================
  # Observability Stack - Grafana
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: always
    networks:
      - chaty-dev
    ports:
      - "3100:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=
    volumes:
      - ./config/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml:ro
      - ./config/grafana-provisioning-dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml:ro
      - grafana_data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_started
      jaeger:
        condition: service_started
      loki:
        condition: service_started
    healthcheck:
      test:
        ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.3"

networks:
  chaty-dev:
    driver: bridge

volumes:
  scylladb_data:
  cockroach_data:
  dragonfly_data:
  redpanda_data:
  minio_data:
  hydra_db_data:
  prometheus_data:
  jaeger_data:
  loki_data:
  grafana_data:
